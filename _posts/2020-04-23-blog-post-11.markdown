---
layout: post
title:  "Blog Post 11"
date:  2020-04-23 0:10:25 -0800
categories: jekyll blog
---
For the twelfth week, I worked on completing an AWS (Amazon Web Services) tutorial designed to educate me as to how the CI/CD Pipeline operates on data and interacts with various components of cloud infrastructure. I now have a broader understanding of the subject matter and the segments involved thanks to the tutorial located here: https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html. The pipeline is essentially composed of three to four parts that work together in tandem. Each component performs an important function, and almost every part builds upon the foundation that was established by the previous component. These services are the CodeCommit data repository, CodeDeploy implementation agent, and of course the integrated pathway known as the CodePipeline. Apparently in some cases, this infrastructure may also include the CodeBuild assembly agent to develop the web application before it is utilized in an active browser session. I started the tutorial by constructing a blank repository within CodeCommit, then I created a remote directory to hold the contents of the files on my local machine. This remote directory was synchronized with the online repository by using the "git clone ssh://git-codecommit.us-west-2.amazonaws.com/v1/repos/MyDemoRepo" command to connect the two storage containers to each other. The command resulted in a sub-directory, named after the repository, being created within the remote directory. It is utilized by the terminal interface in both pushing and pulling operations in which data flows between the directory and the repository. In the second phase of steps, I utilized the sample data that was provided in the listed instructions. These sample files were immediately uploaded to the repository using the "git add -A", "git commit -m 'Add sample application files'", and "git push" set of commands. They prepared the application associated files for delivery, generated a descriptive message intended to notify users of the repository as to what change has occured, and actually moved the files into the storage unit to temporarily hold the information so it can be implemented later on down the line. During the third phase, I configured an EC2 instance to act as a web server for the program. I followed the given steps and inserted a Bash script while progressing through the setup process. The small amount of code in the provided excerpt updated multiple software packages on the sever and installed services such as Ruby, the AWS Command Line Interface, as well as the CodeDeploy agent. From that point, I had to form a new security group that would allow certain web traffic to circulate throughout my network architecture. This security group focused on SSH and HTTP protocols and was assigned to the instance. I had to then enable the secure encryption functionality of the server by specifying a valid key pair. With the instance setup and running, I was now able to proceed to the next phase of detailed instructions. The fourth phase was primarily centered on further developing the CodeDeploy agent, initializing the application data, and setting up the deployment group from within the console. I went through each step so that the CodeDeploy service was ready to perform based on my customized selection of the provided features. The managed policy  AWSCodeDeployRole was setup as an IAM role to give the service the necessary credentials to run properly. It is from there that I had to move onto the initialization part of the section. Under the Applications menu option, I linked the application code to the EC2 instance within the CodeDeploy console. This connection was further solidified when I configured the deployment group that specified how to launch the instance with the CodeDeploy web service. The deployment group did not require any load balancers or system alarms since the tutorial I was following was simplified in nature. At this point, I had to start assembling the actual pipeline that would behave as a sort of pathway for the application data to flow through until the files reached their intended destination. I setup the CodeCommit repository as the source stage with the correct settings as to allow it to detect changes and push new data onto the deployment stage. This permitted CloudWatch events to monitor and manage the specific data passing through the repository as to ensure it is up to date. I skipped the build stage since it was not included in this tutorial, and moved onto the deploy stage. I set the implementation service and its deployment group in the required fields of the CodeDeploy segment. With all of this accomplished, I was then able to activate the codepipeline and bring it into existence. I had to wait for a brief amount time for the codepipeline to load and become fully operational. Once it was ready, I clicked on the details of the deployment stage and the instance ID of the web server. This link sent me to my EC2 Dashboard where I could see the number of running instances within the Management Console. I copied the Public DNS listing into the URL field of a new browsser tab to open a new window for my web application. It loaded correctly and displayed some information about the CodeDeploy service in the AWS documentation. In the sixth phase, I had to update my application code so it would reflect this change in my web browser implementation. I was directed to modify the data inside of the index.html file as to change the type of features and information being displayed on the webpage. Saving this modified code greatly altered the appearance and overall composition of the application. The edits resulted in the inclusion of links to AWS documentation, different text, and a light green background color. The newly integrated links covered material about the CodeCommit and Codepipeline resources in more detail. The exhibition of this upgraded application in the web browser tab demonstrated a successful execution of the code within the Codepipeline. The seventh phase listed instructions and provided links to documentation that explained how to clean up the primary resources and cloud infrastructure utilized in the Management Console by deleting them. The removal of the various resources that were implemented during this tutorial is supposed to clear up my AWS account and prevent me from incurring any uninteded costs that might arise from automatic usage and prolonged existence. Reserved space can be re-opened to me as the user for future activites. This phase concluded the tutorial because the availalbe information in the eighth phase just discussed related material that was not directly connected to the guide about the pipeline. I will  investigate the solutions that are referenced in the seventh phase later on as to explore the possibilities they can provide me. For now, I am attempting to closely analyze the different segments of the CI/CD Pipeline, so I can somehow try to adapt certain aspects of it to my Terraform code for the Codepipeline resource. 
